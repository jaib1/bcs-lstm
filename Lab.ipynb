{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  BCS Computational Tutorial: LSTM Sentiment Classifier\n",
    "\n",
    "We'll be building a tweet sentiment classifier by representing tweets as *sequences* of words, with a LSTM.\n",
    "\n",
    "First import some things and go over the basics of TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle as p\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "import utils\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, check which devices are available to tensorflow, and select the device you want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device List:\n",
      " [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')] \n",
      "\n",
      "\n",
      "Device Details:\n",
      " [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2695590965871351023\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6614766059\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14129835957610308328\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2070, pci bus id: 0000:2d:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6663627080\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8645950860784130695\n",
      "physical_device_desc: \"device: 1, name: GeForce GTX 1080, pci bus id: 0000:23:00.0, compute capability: 6.1\"\n",
      "] \n",
      "\n",
      "\n",
      "Runtime Metadata Last Node Details:\n",
      " MatMul/_0__cf__0   /job:localhost/replica:0/task:0/device:GPU:1\n"
     ]
    }
   ],
   "source": [
    "# Set logging output to display during runtime. (n.b. doesn't work in Jupyter)\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "# Print device list and detailed info about devices.\n",
    "print('Device List:\\n', tf.config.experimental.list_physical_devices(), '\\n\\n')\n",
    "print('Device Details:\\n', device_lib.list_local_devices(), '\\n\\n')\n",
    "# Select device to use, set-up computation graph, and run computation graph.\n",
    "with tf.device('/device:GPU:1'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "    sess = tf.Session()\n",
    "    options = tf.RunOptions(output_partition_graphs=True)\n",
    "    metadata = tf.RunMetadata()\n",
    "    c_val = sess.run(c, options=options, run_metadata=metadata)\n",
    "    last_node = metadata.partition_graphs[0].node[-1]\n",
    "    print('Runtime Metadata Last Node Details:\\n', last_node.name, ' ', last_node.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to TensorFlow\n",
    "\n",
    "## What is a Computation Graph?\n",
    "\n",
    "Everything in TensorFlow comes down to building a computation graph. What is a computation graph? Its just a series of math operations that occur in some order. Here is an example of a simple computation graph:\n",
    "\n",
    "<img src=\"computation-graph.png\">\n",
    "\n",
    "This graph takes 2 inputs, (a, b) and computes an output (e). Each node in the graph is an operation that takes some input, does some computation, and passes its output to another node.\n",
    "\n",
    "We could make this computation graph in TensorFlow in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "c = tf.add(a, b)\n",
    "d = tf.subtract(b, 1)\n",
    "e = tf.multiply(c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow uses tf.placeholder to handle inputs to the model. This is like making a reservation at a restaurant. The restaurant reserves a spot for 5 people, but you are free to fill those seats with any set of friends you want to. tf.placeholder lets you specify that some input will be coming in, of some shape and some type. Only when you run the computation graph do you actually provide the values of this input data. You would run this simple computation graph like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45.0]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    a_data, b_data = 3.0, 6.0\n",
    "    feed_dict = {a: a_data, b: b_data}\n",
    "    output = session.run([e], feed_dict=feed_dict)\n",
    "    print(output) # 45.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use feed_dict to pass in the actual input data into the graph. We use session.run to get the output from the c operation in the graph. Since e is at the end of the graph, this ends up running the entire graph and returning the number 45 - cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks in Tensorflow\n",
    "\n",
    "We can define neural networks in TensorFlow using computation graphs. Here is an example, very simple neural network (just 1 perceptron):\n",
    "\n",
    "<img src=\"computation-graph-2.png\">\n",
    "\n",
    "This graph takes an input, (x) and computes an output (out). It does it with what we learned in class, `out = sigmoid(W*x+b)`.\n",
    "\n",
    "We could make this computation graph in TensorFlow in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input_nodes = 2\n",
    "n_output_nodes = 1\n",
    "x = tf.placeholder(tf.float32, (None, n_input_nodes))\n",
    "W = tf.Variable(tf.ones((n_input_nodes, n_output_nodes)), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros(n_output_nodes), dtype=tf.float32)\n",
    "\n",
    "'''TODO: Define the operation for z (use tf.matmul to multiply W and x).'''\n",
    "z = tf.matmul(x,W) + b\n",
    "\n",
    "'''TODO: Define the operation for out (use tf.sigmoid).'''\n",
    "out = tf.sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this graph, we again use session.run() and feed in our input via feed_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7310586]]\n"
     ]
    }
   ],
   "source": [
    "test_input = [[0.5, 0.5]]\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run(session=session)\n",
    "    feed_dict = {x: test_input}\n",
    "    output = session.run([out], feed_dict=feed_dict)\n",
    "    print(output[0]) # This should output 0.73105. If not, double-check your code above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LSTMs for Tweet Sentiment Classification\n",
    "Our model will be like this:\n",
    "\n",
    "![alt-text](lab2-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed words one by one into LSTM layers.  After feeding in all the words, we take the final state of the LSTM and run it thorugh one fully connected layer to multiply it by a final set of weights. We specificy that this fully connected layer should have a single output, which, when sigmoid-ed, is the probability that the tweet is positive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up our Model Parameters\n",
    "\n",
    "Similarly to the last lab, we'll be training using batches. Our hidden layer will have 100 units, and we have 7597 words in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "tweet_size = 20\n",
    "hidden_size = 100\n",
    "vocab_size = 7597\n",
    "\n",
    "# this just makes sure that all our following operations will be placed in the right graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# create a session variable that we can run later.\n",
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Placeholders\n",
    "\n",
    "We need to create placeholders for variable data that we will feed in ourselves (aka our tweets). Placeholders allow us to incorporate this data into the graph even though we don't know what it is yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the placeholder for tweets has first dimension batch_size for each tweet in a batch,\n",
    "# second dimension tweet_size for each word in the tweet, and third dimension vocab_size\n",
    "# since each word itself is represented by a one-hot vector of size vocab_size.\n",
    "# Note that we use 'None' instead of batch_size for the first dimsension.  This allows us \n",
    "# to deal with variable batch sizes\n",
    "tweets = tf.placeholder(tf.float32, [None, tweet_size, vocab_size])\n",
    "\n",
    "'''TODO: create a placeholder for the labels (our predictions).  \n",
    "   This should be a 1D vector with size = None, \n",
    "   since we are predicting one value for each tweet in the batch,\n",
    "   but we want to be able to deal with variable batch sizes.''';\n",
    "labels = tf.placeholder(tf.float32, [None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build the LSTM Layers\n",
    "\n",
    "We want to feed the input sequence, word by word, into an LSTM layer, or multiple LSTM layers (we could also call this an LSTM **encoder**).  At each \"timestep\", we feed in the next word, and the LSTM updates its cell state. The final LSTM cell state can then be fed through a final classification layer(s) to get our sentiment prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make our LSTM layer. The steps for this are: \n",
    "1. Create a LSTM Cell using [tf.contrib.rnn.LSTMCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell)\n",
    "\n",
    "2. Wrap a couple of these cells in [tf.nn.rnn_cell.MultiRNNCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell) to create a multiple LSTM layers.\n",
    "\n",
    "2. Define the operation to run these layers with [dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-9-00d97ee18518>:3: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-00d97ee18518>:11: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-00d97ee18518>:16: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\Jai\\Anaconda3\\envs\\intro_to_tf_lstm\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Jai\\Anaconda3\\envs\\intro_to_tf_lstm\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "'''TODO: create 2 LSTM Cells using BasicLSTMCell.  Note that this creates a *layer* of LSTM\n",
    "   cells, not just a single one.''';\n",
    "lstm_cell_1 = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "lstm_cell_2 = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "\n",
    "'''TODO: create two LSTM layers by wrapping two instances of \n",
    "   lstm_cell from above in tf.contrib.rnn_cell.MultiRNNCell. Note that\n",
    "   you can use multiple cells by doing [cell1, cell2]. Also note\n",
    "   that you should use state_is_tuple=True as an argument.  This will allow\n",
    "   us to access the part of the cell state that we need later on.''';\n",
    "multi_lstm_cells = multi_lstm_cells = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell_1, lstm_cell_2] , state_is_tuple=True)\n",
    "\n",
    "'''TODO: define the operation to create the RNN graph across time.  \n",
    "   tf.nn.dynamic_rnn dynamically constructs the graph when it is executed,\n",
    "   and returns the final cell state.''';\n",
    "outputs, final_state = tf.nn.dynamic_rnn(multi_lstm_cells, tweets, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Classification Layer\n",
    "\n",
    "Now we have the final state of the LSTM layers after feeding in the tweet word by word. We can take this final state and feed it into a simple classfication layer that takes the cell state, multiplies it by some weight matrix (with bias) and outputs a single value corresponding to whether it thinks the tweet is overall positive or not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We define this function that creates a weight matrix + bias parameter\n",
    "## and uses them to do a matrix multiplication.\n",
    "def linear(input_, output_size, name, init_bias=0.0):\n",
    "    shape = input_.get_shape().as_list()\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable(\"weights\", [shape[-1], output_size], tf.float32, tf.random_normal_initializer(stddev=1.0 / math.sqrt(shape[-1])))\n",
    "    if init_bias is None:\n",
    "        return tf.matmul(input_, W)\n",
    "    with tf.variable_scope(name):\n",
    "        b = tf.get_variable(\"bias\", [output_size], initializer=tf.constant_initializer(init_bias))\n",
    "    return tf.matmul(input_, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO: pass the final state into this linear function to multiply it \n",
    "   by the weights and add bias to get our output.\n",
    "   \n",
    "   {Quick note that we need to feed in final_state[-1][-1] into linear since \n",
    "   final_state is actually a tuple consisting of the cell state \n",
    "   (used internally for the cell to keep track of things) \n",
    "   as well as the hidden state (the output of the cell), and one of these \n",
    "   tuples for each layer. We want the hidden state for the last layer, so we use \n",
    "   final_state[-1][-1]}''';\n",
    "\n",
    "sentiment = linear(final_state[-1][-1], 1, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Loss\n",
    "\n",
    "Now we define a loss function that we'll use to determine the difference between what we predicted and what's actually correct.  We'll want to use cross entropy, since we can take into account what probability the model gave to the a tweet being positive.\n",
    "\n",
    "The output we just got from the linear classification layer is called a 'logit' -- the raw value before transforming it into a probability between 0 and 1.  We can feed these logits to  [`tf.nn.sigmoid_cross_entropy_with_logits`](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits), which will take the sigmoid of these logits (making them between 0 and 1) and then calculate the cross-entropy with the ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jai\\Anaconda3\\envs\\intro_to_tf_lstm\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From <ipython-input-12-d9e1456c3da9>:15: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "sentiment = tf.squeeze(sentiment, [1])\n",
    "\n",
    "'''TODO: define our loss function.  \n",
    "   We will use tf.nn.sigmoid_cross_entropy_with_logits, which will compare our \n",
    "   sigmoid-ed prediction (sentiment from above) to the ground truth (labels).''';\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=sentiment, labels=labels)\n",
    "\n",
    "# our loss with sigmoid_cross_entropy_with_logits gives us a loss for each \n",
    "# example in the batch. We take the mean of all these losses.\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# to get actual results like 'positive' or 'negative' , \n",
    "# we round the prediction probability to 0 or 1.\n",
    "prediction = tf.to_float(tf.greater_equal(sentiment, 0.5))\n",
    "\n",
    "# calculate the error based on which predictions were actually correct.\n",
    "pred_err = tf.to_float(tf.not_equal(prediction, labels))\n",
    "pred_err = tf.reduce_sum(pred_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train\n",
    "\n",
    "Now we define the operation that actually changes the weights by minimizing the loss.  \n",
    "\n",
    "[`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) is just a gradient descent algorithm that uses a variable learning rate to converge faster and more effectively.\n",
    "\n",
    "We want to specify this optimizer and then call the [minimize](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer#minimize) function, the optimizer knows it wants to minimize the loss we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define the operation that specifies the AdamOptimizer and tells\n",
    "   it to minimize the loss.''';\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run Session!\n",
    "\n",
    "Now that we've made all the variable and operations in our graph, we can load the data, feed it in..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize any variables\n",
    "tf.global_variables_initializer().run(session=session)\n",
    "\n",
    "# load our data and separate it into tweets and labels\n",
    "train_data = json.load(open('data/trainTweets_preprocessed.json', 'r'))\n",
    "train_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),train_data))\n",
    "train_tweets = np.array([t[0] for t in train_data])\n",
    "train_labels = np.array([int(t[1]) for t in train_data])\n",
    "\n",
    "test_data = json.load(open('data/testTweets_preprocessed.json', 'r'))\n",
    "test_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),test_data))\n",
    "# we are just taking the first 1000 things from the test set for faster evaluation\n",
    "test_data = test_data[0:1000] \n",
    "test_tweets = np.array([t[0] for t in test_data])\n",
    "one_hot_test_tweets = utils.one_hot(test_tweets, vocab_size)\n",
    "test_labels = np.array([int(t[1]) for t in test_data])\n",
    "\n",
    "# we'll train with batches of size 32. This means that we run \n",
    "# our model on 32 examples and then do gradient descent based on the loss\n",
    "# over those 32 examples.\n",
    "batch_size = 32\n",
    "num_steps = np.int(len(train_data) / batch_size)\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent in last epoch:  80.31361320000002\n",
      "Train loss at epoch 0 : 0.43616664\n",
      "Train error: 8.000%\n",
      "Test loss: 0.513\n",
      "Test error: 8.774%\n",
      "Time spent in last epoch:  161.97296820000003\n",
      "Train loss at epoch 1 : 0.35408577\n",
      "Train error: 4.000%\n",
      "Test loss: 0.517\n",
      "Test error: 8.645%\n",
      "Time spent in last epoch:  243.72728009999997\n",
      "Train loss at epoch 2 : 0.25547802\n",
      "Train error: 4.000%\n",
      "Test loss: 0.586\n",
      "Test error: 8.516%\n",
      "Time spent in last epoch:  325.6937339\n",
      "Train loss at epoch 3 : 0.23870307\n",
      "Train error: 5.000%\n",
      "Test loss: 0.724\n",
      "Test error: 9.323%\n",
      "Time spent in last epoch:  406.9539039\n",
      "Train loss at epoch 4 : 0.15966126\n",
      "Train error: 3.000%\n",
      "Test loss: 0.727\n",
      "Test error: 9.710%\n",
      "Time spent training:  407.63247879999994\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "for epoch in range(num_epochs):\n",
    "    for step in range(num_steps):\n",
    "        # get data for a batch\n",
    "        offset = (step * batch_size) % (len(train_data) - batch_size)\n",
    "        batch_tweets = utils.one_hot(train_tweets[offset : (offset + batch_size)], vocab_size)\n",
    "        batch_labels = train_labels[offset : (offset + batch_size)]\n",
    "\n",
    "        # put this data into a dictionary that we feed in when we run \n",
    "        # the graph. This data fills in the placeholders we made in the graph.\n",
    "        data = {tweets: batch_tweets, labels: batch_labels}\n",
    "\n",
    "        # run the 'optimizer', 'loss', and 'pred_err' operations in the graph\n",
    "        _, loss_value_train, error_value_train = session.run(\n",
    "          [optimizer, loss, pred_err], feed_dict=data)\n",
    "    \n",
    "    # print stuff every epoch to see how we are doing\n",
    "    print(\"Time spent in last epoch: \", time.perf_counter() - t0)\n",
    "    print(\"Train loss at epoch\", epoch, \":\", loss_value_train)\n",
    "    print(\"Train error: %.3f%%\" % error_value_train)\n",
    "\n",
    "    # get test evaluation\n",
    "    test_loss = []\n",
    "    test_error = []\n",
    "    for batch_num in range(int(len(test_data)/batch_size)):\n",
    "        test_offset = (batch_num * batch_size) % (len(test_data) - batch_size)\n",
    "        test_batch_tweets = one_hot_test_tweets[test_offset : (test_offset + batch_size)]\n",
    "        test_batch_labels = test_labels[test_offset : (test_offset + batch_size)]\n",
    "        data_testing = {tweets: test_batch_tweets, labels: test_batch_labels}\n",
    "        loss_value_test, error_value_test = session.run([loss, pred_err], feed_dict=data_testing)\n",
    "        test_loss.append(loss_value_test)\n",
    "        test_error.append(error_value_test)\n",
    "\n",
    "    print(\"Test loss: %.3f\" % np.mean(test_loss))\n",
    "    print(\"Test error: %.3f%%\" % np.mean(test_error))\n",
    "print(\"\\nTime spent training: \", time.perf_counter() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this code, you’ll see the network train and output its performance as it learns. \n",
    "\n",
    "## Concluding Thoughts\n",
    "This was a brief introduction into TensorFlow. There is so, so much more to learn and explore, but hopefully this has given you some base knowledge to expand upon. As an additional exercise, you can see what you can do with this code to improve the performance. Ideas include: randomizing mini-batches, making the network deeper, using word embeddings (see below) rather than bag-of-words, trying different optimizers (like Adam), different weight initializations.\n",
    "\n",
    "#### More on Word Embeddings\n",
    "\n",
    "In this lab we used one-hot vectors to represent words in tweets.  Word Embeddings are a more meaningful representation.  The basic idea is we represent a word with a vector $\\phi$ by the context the word appears in. We do this by training a neural network to predict the context of words across a large training set. The weights of that neural network can then be thought of as a dense and useful representation that captures context. This is useful because now our representations of words captures actual semantic similarity.\n",
    "\n",
    "Word Embeddings capture all kinds of useful semantic relationships. For example, one cool emergent property is $ \\phi(king) - \\phi(queen) = \\phi(man) - \\phi(woman)$. To learn more about the magic behind word embeddings we recommend Chris Olah's [blog post](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/). A common tool for generating Word Embeddings is word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle as p\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils \n",
    "\n",
    "# set variables\n",
    "tweet_size = 20\n",
    "hidden_size = 100\n",
    "vocab_size = 7597\n",
    "batch_size = 64\n",
    "\n",
    "# this just makes sure that all our following operations will be placed in the right graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# create a session variable that we can run later.\n",
    "session = tf.Session()\n",
    "\n",
    "# make placeholders for data we'll feed in\n",
    "tweets = tf.placeholder(tf.float32, [None, tweet_size, vocab_size])\n",
    "labels = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "# make the lstm cells, and wrap them in MultiRNNCell for multiple layers\n",
    "lstm_cell_1 = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "lstm_cell_2 = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "multi_lstm_cells = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell_1, lstm_cell_2] , state_is_tuple=True)\n",
    "\n",
    "# define the op that runs the LSTM, across time, on the data\n",
    "outputs, final_state = tf.nn.dynamic_rnn(multi_lstm_cells, tweets, dtype=tf.float32)\n",
    "\n",
    "# a useful function that takes an input and what size we want the output \n",
    "# to be, and multiples the input by a weight matrix plus bias (also creating\n",
    "# these variables)\n",
    "def linear(input_, output_size, name, init_bias=0.0):\n",
    "    shape = input_.get_shape().as_list()\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable(\"weight_matrix\", [shape[-1], output_size], tf.float32, tf.random_normal_initializer(stddev=1.0 / math.sqrt(shape[-1])))\n",
    "    if init_bias is None:\n",
    "        return tf.matmul(input_, W)\n",
    "    with tf.variable_scope(name):\n",
    "        b = tf.get_variable(\"bias\", [output_size], initializer=tf.constant_initializer(init_bias))\n",
    "    return tf.matmul(input_, W) + b\n",
    "\n",
    "# define that our final sentiment logit is a linear function of the final state \n",
    "# of the LSTM\n",
    "sentiment = linear(final_state[-1][-1], 1, name=\"output\")\n",
    "\n",
    "\n",
    "sentiment = tf.squeeze(sentiment, [1])\n",
    "\n",
    "# define cross entropy loss function\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=sentiment, labels=labels)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# round our actual probabilities to compute error\n",
    "prob = tf.nn.sigmoid(sentiment)\n",
    "prediction = tf.to_float(tf.greater_equal(prob, 0.5))\n",
    "pred_err = tf.to_float(tf.not_equal(prediction, labels))\n",
    "pred_err = tf.reduce_sum(pred_err)\n",
    "\n",
    "# define our optimizer to minimize the loss\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# initialize any variables\n",
    "tf.global_variables_initializer().run(session=session)\n",
    "\n",
    "# load our data and separate it into tweets and labels\n",
    "train_data = json.load(open('data/trainTweets_preprocessed.json', 'r'))\n",
    "train_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),train_data))\n",
    "\n",
    "train_tweets = np.array([t[0] for t in train_data])\n",
    "train_labels = np.array([int(t[1]) for t in train_data])\n",
    "\n",
    "test_data = json.load(open('data/testTweets_preprocessed.json', 'r'))\n",
    "test_data = map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),test_data)\n",
    "# we are just taking the first 1000 things from the test set for faster evaluation\n",
    "test_data = test_data[0:1000] \n",
    "test_tweets = np.array([t[0] for t in test_data])\n",
    "one_hot_test_tweets = utils.one_hot(test_tweets, vocab_size)\n",
    "test_labels = np.array([int(t[1]) for t in test_data])\n",
    "\n",
    "# we'll train with batches of size 128.  This means that we run \n",
    "# our model on 128 examples and then do gradient descent based on the loss\n",
    "# over those 128 examples.\n",
    "num_steps = 1000\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # get data for a batch\n",
    "    offset = (step * batch_size) % (len(train_data) - batch_size)\n",
    "    batch_tweets = utils.one_hot(train_tweets[offset : (offset + batch_size)], vocab_size)\n",
    "    batch_labels = train_labels[offset : (offset + batch_size)]\n",
    "    \n",
    "    # put this data into a dictionary that we feed in when we run \n",
    "    # the graph.  this data fills in the placeholders we made in the graph.\n",
    "    data = {tweets: batch_tweets, labels: batch_labels}\n",
    "    \n",
    "    # run the 'optimizer', 'loss', and 'pred_err' operations in the graph\n",
    "    _, loss_value_train, error_value_train = session.run(\n",
    "      [optimizer, loss, pred_err], feed_dict=data)\n",
    "    \n",
    "    # print stuff every 50 steps to see how we are doing\n",
    "    if (step % 50 == 0):\n",
    "        print(\"Minibatch train loss at step\", step, \":\", loss_value_train)\n",
    "        print(\"Minibatch train error: %.3f%%\" % error_value_train)\n",
    "        \n",
    "        # get test evaluation\n",
    "        test_loss = []\n",
    "        test_error = []\n",
    "        for batch_num in range(int(len(test_data)/batch_size)):\n",
    "            test_offset = (batch_num * batch_size) % (len(test_data) - batch_size)\n",
    "            test_batch_tweets = one_hot_test_tweets[test_offset : (test_offset + batch_size)]\n",
    "            test_batch_labels = test_labels[test_offset : (test_offset + batch_size)]\n",
    "            data_testing = {tweets: test_batch_tweets, labels: test_batch_labels}\n",
    "            loss_value_test, error_value_test = session.run([loss, pred_err], feed_dict=data_testing)\n",
    "            test_loss.append(loss_value_test)\n",
    "            test_error.append(error_value_test)\n",
    "        \n",
    "        print(\"Test loss: %.3f\" % np.mean(test_loss))\n",
    "        print(\"Test error: %.3f%%\" % np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
